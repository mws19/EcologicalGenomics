# Morgan -- Nice job on your notebook. Most of your entires are well organized, include helpful annotations to describe what you did and why, and provide the command-line code needed for reproducing your work. Nice job. --Steve


---
title: "Ecological Genomics Lab Notebook"
author: "Morgan Southgate"
output:
  html_document:
    highlight: tango
    theme: cerulean
  pdf_document: default
---
### Overall Description of notebook      

This notebook will catalog my entire work throughout the semester, including all my relevant code, results, and interpretations

### Date started: January 24, 2018   
### Date end:   (year-month-day)    

### Philosophy   
Science should be reproducible and one of the best ways to achieve this is by logging research activities in a notebook. Because science/biology has increasingly become computational, it is easier to document computational projects in an electronic form, which can be shared online through Github.    

### Helpful features of the notebook     

**It is absolutely critical for your future self and others to follow your work.**     

* The notebook is set up with a series of internal links from the table of contents.    
* All notebooks should have a table of contents which has the "Page", date, and title (information that allows the reader to understand your work).     
* Also, one of the perks of keeping all activities in a single document is that you can **search and find elements quickly**.     
* Lastly, you can share specific entries because of the three "#" automatically creates a link when the notebook renders on github.      


<a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.  


### Table of contents for 60 entries (Format is Page: Date(with year-month-day). Title)        
* [Page 1: 2018-01-24](#id-section1). Intro to Github, RMarkdown, and UNIX command-line
* [Page 2: 2018-01-29](#id-section2). Practice logging into UNIX and basic coding.
* [Page 3: 2018-01-29](#id-section3). Working with RNA-Seq Data
* [Page 4: 2018-01-31](#id-section4). Continuing to work with RNA-Seq Data
* [Page 5: 2018-02-05](#id-section5). Continuing to work with the RNA-Seq Data
* [Page 6: 2018-02-07](#id-section6).Working with DESeq2
* [Page 7: 2018-02-12](#id-section7).Continuing to work with DESeq2. 
* [Page 8: 2018-02-14](#id-section8). FInishing work with DESeq2. 
* [Page 9: 2018-02021](#id-section9).DESeq2 one more time!
* [Page 10: 2018-02-26](#id-section10). Population Genomics 1: Intro to SNP and genotype calling
* [Page 11:2018-02-28](#id-section11).Population Genomics 2: Diversity and Site Frequency Spectrum (SFS) 
* [Page 12: 2018-03-05](#id-section12).Admixture and Population Structure
* [Page 13: 2018-03-07](#id-section13).Admixture & Population Structure Continued
* [Page 14: 2018-03-08](#id-section14). R Script for HW1 - Differential Gene Expression 
* [Page 15: 2018-03-19](#id-section15). Admixture & Population Structure Continued
* [Page 16: 2018-03-21](#id-section16). Testing Selection Using Fst Outlier Analysis
* [Page 17: 2018-03-26](#id-section17). Population Genomics 7: Analyzing Fst Outlier (Bayescan) results
* [Page 18: 2018-03-28](#id-section18). Population Genomics 7 Continued
* [Page 19: 2018-04-16](#id-section19). Work on DemographicHistory Project
* [Page 20: 2018-04-18](#id-section20). Work on DemHist Project continued.
* [Page 21: 2018-04-23](#id-section21). Continuing work on DemHist Project. 
* [Page 22:](#id-section22).
* [Page 23:](#id-section23).
* [Page 24:](#id-section24).
* [Page 25:](#id-section25).
* [Page 26:](#id-section26).
* [Page 27:](#id-section27).
* [Page 28:](#id-section28).
* [Page 29:](#id-section29).
* [Page 30:](#id-section30).
* [Page 31:](#id-section31).
* [Page 32:](#id-section32).
* [Page 33:](#id-section33).
* [Page 34:](#id-section34).
* [Page 35:](#id-section35).
* [Page 36:](#id-section36).
* [Page 37:](#id-section37).
* [Page 38:](#id-section38).
* [Page 39:](#id-section39).
* [Page 40:](#id-section40).
* [Page 41:](#id-section41).
* [Page 42:](#id-section42).
* [Page 43:](#id-section43).
* [Page 44:](#id-section44).
* [Page 45:](#id-section45).
* [Page 46:](#id-section46).
* [Page 47:](#id-section47).
* [Page 48:](#id-section48).
* [Page 49:](#id-section49).
* [Page 50:](#id-section50).
* [Page 51:](#id-section51).
* [Page 52:](#id-section52).
* [Page 53:](#id-section53).
* [Page 54:](#id-section54).
* [Page 55:](#id-section55).
* [Page 56:](#id-section56).
* [Page 57:](#id-section57).
* [Page 58:](#id-section58).
* [Page 59:](#id-section59).
* [Page 60:](#id-section60).

------
<div id='id-section1'/>
### Page 1: 2018-01-24. Notes on using Github, Rmarkdown, and the UNIX command-line

Today, we created our github repos for the course, and began our notebooks. 

Other goals for today: 
* publish our notebooks
* log into the UNIX server

------
<div id='id-section2'/>
### Page 2: 2018-01-29. Practice logging into UNIX and basic coding.
Logged into UNIX server using Putty, accessed mydata, viewed cols_data.txt

------
<div id='id-section3'/>
### Page 3: 2018-01-29. Working with RNA-Seq Data. 

Goals: 
- Learn about the bull headed beetle (*Onthophagus taurus*)
- Understand pipeline for processing RNA-Seq data
- Learn how to write bash scripts and write scripts to process files in batches
 *.sh files
 *#! notation
- Visualize and interpret Illumina data quality
 * fastq files
 * Phred scores


Bull headed beetle (*Onthophagus taurus*)
- Native to mediterranean, deliberately introduced to Australia for pest control
- Accidentally introduced to eastern US from unknown origin in 1970s
- Experimental design:
   * Three populations reared in common garden experiment, from native range(Mediterranean), Italy (IT), western australia (WA), and North Carolina (NC)
   * Four developmental stages L3L (late third larval insert), PP1 (pre-pupae day 1), PD1 (pupae day 1), and AD4 (adults 4 days after ecolsion)
   * Both sexes (three individuals per sex)
   * 3 pops x 4 developmental stages x 2 sexes x 3 individuals = 72 samples
   * Sequenced on about 7 lanes of Illumina HiSeq 2500

## The pipeline for processing transcriptomic data
1. Visualize, Clean, Visualize
- Visualize data quality using the FastQC program
- Clean raw data using the Trimmomatic program
- Visualize quality of cleaned data using FastQC
2. Download reference transcriptome assembly
3. Map (aka align) cleaned reads from each sample to the reference assembly to generate sequence alignemnt files (sam files) (Program: bwa, Input: fastq, Output: .sam)
4. Extract read count data from .sam files (the number of reads that map to each "gene")
5. Assemble a data matrix of counts for each gene for each sample
6. Analyze count data to test for differences in expression

## 1. Visualize,Clean,Visualize
- Access shared directory through pbio381 server
- Working with WA_PP1_M1 - open file and check top four lines
- Get Quality score output
- Clean file using fastqc, save output in homedirectory/mydata
```
cd /data/project_data/beetles/rawdata/
zcat WA_PP1_M1_CCGTCC_L003_R1_001.fastq.gz | head -n4
@@@DDDDDA?DCFEGGEHHHGGEGGII+C1?EHBHGG##0:BDDFGIIDHG1=CGIGGGEHB3?D>CCAA;?:>>?BA@CB@A>ACCCBBB#+8?BBC##
WA_PP1_M1_CCGTCC_L003_R1_001.fastq.gz -o ~/mydata
```
- Move html file generated to documents/EcologicalGenomics/ using WINSCP
- Opened fastq file in html document

## What is a FastQ file?
The fastq file format has 4 lines for each read: 

Line | Description
-----|------------
1    | Begins with '@' and then information about the read
2    | The actual DNA sequence
3    | Begins with '+' and sometimes same info as line 1
4    | A string of characters representing the quality score, always with same number of characters as line 2

## What is a Phred Quality Score (Q Score)?
 [Useful reference for Phred Q scores](http://www.drive5.com/usearch/manual/quality_score.html)
 
 P = 10^(-Q/10)
 Q = -10log10(P)
 
 Phred Q Score | Prob Incorrect Base Call| Base Call Accuracy
 --------------|-------------------------|--------------------
 10            | 1 in 10                 |   90%
 20            | 1 in 100                |   99%
 30            | 1 in 1000               | 99.9%
 40            | 1 in 10000              |99.99%
 
 The Phred Q Score is translated to ASCII characters, meaning that a two digit number can be represented by a single character: 

Quality encoding: !"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHI
                  |         |         |         |         |
   Quality score: 0........10........20........30........40  

------
<div id='id-section4'/>
### Page 4: Continuing to work with RNA-Seq Data

#1. Visualize, Clean, and Visualize Again Continued

- Clean reads using Trimmomatic
1. Make directories "scripts" and "cleanreads"
2. Copy bash script over to ~/scripts directory 

```
cp /data/scripts/trim_example.sh ~/scripts/ # copies the script to your home scripts dir
vim trim_example.sh                       # open the script with vim to edit
```

3. Open and edit the bash script using the program vim
4. Change the permissions on the script to make it executable, then run it
 *Trimmomatic performs cleaning steps in order presented - recommended to clip adapter early in process and clean for length at the end. 
 * The steps and options are from [the trimmomatic website](http://www.usadellab.org/cms/index.php?page=trimmomatic)
 
ILLUMINACLIP::::
SLIDINGWINDOW::
windowSize: specifies the number of bases to average across
requiredQuality: specifies the average quality required.
LEADING:
quality: Specifies the minimum quality required to keep a base.
TRAILING:
quality: Specifies the minimum quality required to keep a base.
CROP:
length: The number of bases to keep, from the start of the read.
HEADCROP:
length: The number of bases to remove from the start of the read.
MINLEN:
length: Specifies the minimum length of reads to be kept.
ILLUMINACLIP: Cut adapter and other illumina-specific sequences from the read.
SLIDINGWINDOW: Perform a sliding window trimming, cutting once the average quality within the window falls below a threshold.
LEADING: Cut bases off the start of a read, if below a threshold quality
TRAILING: Cut bases off the end of a read, if below a threshold quality
CROP: Cut the read to a specified length
HEADCROP: Cut the specified number of bases from the start of the read
MINLEN: Drop the read if it is below a specified length

```
#!/bin/bash
      java -classpath /data/popgen/Trimmomatic-0.33/trimmomatic-0.33.jar org.usadellab.trimmomatic.TrimmomaticPE \  
                -threads 1 \ 
                -phred33 \
                 /data/project_data/beetles/rawdata/WA_PP1_M1_CCGTCC_L003_R1_00_R1.fastq.gz \
                 /data/project_data/beetles/rawdata/WA_PP1_M1_CCGTCC_L003_R1_00_R2.fastq.gz \
                 ~/cleanreads/"WA_PP1_M1_CCGTCC_L003_R1_001_clean_paired.fa" \
                 ~/cleanreads/"WA_PP1_M1_CCGTCC_L003_R1_001_clean_unpaired.fa" \
                 ~/cleanreads/"WA_PP1_M1_CCGTCC_L003_R1_002_clean_paired.fa" \
                 ~/cleanreads/"WA_PP1_M1_CCGTCC_L003_R1_002_clean_unpaired.fa" \
                 ILLUMINACLIP:/data/popgen/Trimmomatic-0.33/adapters/TruSeq3-PE.fa:2:30:10 \
                 LEADING:28 \
                TRAILING:28 \
                SLIDINGWINDOW:6:28 \
                HEADCROP:12 \
                MINLEN:35 \
```
Now run the script
 ```
 chmod u+x trim_example.sh    # makes the script "executable" by the "user"
./trim_example.sh           # executes the script, or bash trim_example.sh
```
Output: 
```
TrimmomaticPE: Started with arguments: -threads 1 -phred33 /data/project_data/beetles/rawdata/WA_PP1_M1_CCGTCC_L003_R1_001.fastq.gz /data/project_data/beetles/rawdata/WA_PP1_M1_CCGTCC_L003_R2_001.fastq.gz /users/m/s/msouthga/cleanreads/WA_PP1_M1_CCGTCC_L003_R1_clean-paired.fa /users/m/s/msouthga/cleanreads/WA_PP1_M1_CCGTCC_L003_R1_clean_unpaired.fa /users/m/s/msouthga/cleanreads/WA_PP1_M1_CCGTCC_L003_R2_clean_paired.fa /users/m/s/msouthga/cleanreads/WA_PP1_M1_CCGTCC_L003_R2_clean_unpaired.fa ILLUMINACLIP:/data/popgen/Trimmomatic-0.33/adapters/TruSeq3-PE.fa:2:30:10 LEADING:28 TRAILING:28 SLIDINGWINDOW:6:28 HEADCROP:12 MINLEN:35
Using PrefixPair: 'TACACTCTTTCCCTACACGACGCTCTTCCGATCT' and 'GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT'
ILLUMINACLIP: Using 1 prefix pairs, 0 forward/reverse sequences, 0 forward only sequences, 0 reverse only sequences
```
 - Run FASTQC on our clean and paired reads, and store in fastqc directory
 - save a [copy](file:///C:/Users/MorganWS/Documents/EcologicalGenomics/Transcriptomics/1-31_RNASeqData/WA_PP1_M1_CCGTCC_L003_R1_clean-paired.fa_fastqc.html) of clean and paired reads as html file in ~EcologicalGenomics/1-31_RNASeq

```
fastqc ~/cleanreads/WA_PP1_M1_CCGTCC_L003_R1_clean-paired.fa -o ~/fastqc/

```

#2: Download reference transcriptome
#3: Map reads to the reference transcriptome
 - First step is to index the reference transcriptome. Done already, but here's the command: 
```
$ bwa index /data/project_data/beetles/reference/OTAU.fna
```
 - This generates 5 indexing files that all start with OTAU.fna in the /reference directory
 
 - make a new directory sam in mydata 
 - Our first step is to map the clean reads using the bwa mem command
 - output is a .sam file
```
bwa mem <ref.fa> <read1.fq> <read2.fq> > <aln-pe.sam> # you fill in the inputs and outputs!
```
```
bwa mem /data/project_data/beetles/reference/OTAU.fna ~/cleanreads/WA_PP1_M1_CCGTCC_L003_R1_clean-paired.fa ~/cleanreads/WA_PP1_M1_CCGTCC_L003_R2_clean_paired.fa > WA_PP1_M1_bwamem.sam
```

------
<div id='id-section5'/>
### Page 5: Continuing to work with RNA-Seq Data

navigate to mydata/sam

```
 tail -n 100 WA_PP1_M1_bwamem.sam > tail.sam   #take 100 last columns in file
 vim tail.sam   #view tail.sam file
 : set nowrap
```
A SAM file is a tab delimited text file that stores information about the alignment of reads in a FASTQ file to a reference gnome or transcriptome. For each read in a FASTQ file, there's a line in the SAM file that includes: 

- the read (aka query name)
- a FLAG (number with info about mapping success and orientation and whether the read is the left or right read)
 * Check FLAG scores using BROAD institue page (https://broadinstitute.github.io/picard/explain-flags.html)
- The reference sequence name to which the read mapped (if not mapped gets a *)
- The mapping quality (Phred-scaled)
- a CIGAR string that gives alignment information (how many bases Match, location of Insertion or Deletion)
- an '=', mate position, inferred insert size(columns 7,8,9)
- the query sequence and Phred-scaled quality from the FASTQ file (columsn 10 and 11)
- then lots of good information in TAGS at the end (if the read mapped) including whether it is a unique read (XT:A:U) the number of best hits (X0:i:1), and the number of suboptimal hits (X1:i:0)

The left (R1) and right (R2) reads alternate throughout the file. SAM files usually have a header section with generation information where each line starts with the '@' symbol. SAM and BAM files contain the same information, but SAM is human readable whereas BAM is in binary code, so has a smaller file size. 

[Here](http://samtools.github.io/hts-specs/SAMv1.pdf) is the official SAM documentation
[Here](https://broadinstitute.github.io/picard/explain-flags.html) is the tool for decoding the numbers in the second column of data
[Here](http://www.acgt.me/blog/2014/12/16/understanding-mapq-scores-in-sam-files-does-37-42) is the reference for interpreting map quality scores

To get a summary of how well the reads mapped to the reference: 
```
samtools flagstat *.sam
```
To see how many of the reads map uniquely: 
* Think about parology!

```
grep -c XT:A:U WA_PP1_M1_bwamem.sam  #use general expressions to search for specific term

samtools flagstat WA_PP1_M1_bwamem.sam

cp /data/scripts/countxpression_PE.py ~/myscripts  #move countxpression_PE.py file from shared directory to myscripts

vim countxpression_PE.py  # visualize file

```

------
<div id='id-section6'/>
### Page 6: 2018-02-07. Working with DESeq2. 
Working with R on personal machine using: 

-counts matrix
-compiled table of uniquely mapped reads to each gene from each sample

DESeq2 - can rerun with bwa align, get python script to work with mem, or choose different way. 
1) start with bwa align. Use WinSCP to transfer file into EcologicalGenomics/UNIXData/Feb7

work recorded in Feb7_DESeq2.R

------
<div id='id-section7'/>
### Page 7: Continuing to work with DESeq2. 


------
<div id='id-section8'/>
### Page 8: Finishing work with DESeq2. 

- Made a heatmap comparing expression levels
- Look at individual genes by devstage and sex using PCA

-GO enrichment using MWU test
------
<div id='id-section9'/>
### Page 9: One more day with DESeq2!
Goals: 
-1) Explore how we can use DESeq2 and other packages in R to test for differences in gene expression of different models
3) To identify co-expression modules using WCGNA

- IT population didn't provide all developmental stages - so missing from some of this analysis

- made heat map of significant expression differences between WA_PP1_F and NC_PP1_F (also with corresponding male samples)

- WGNCA = weighted gene network correlation analysis, performed using package WGNCA. 
- brightest clusters and strongest correlations shown in developmental stage

------
<div id='id-section10'/>
### Page 10: Population Genomics 1: Intro to SNP and genotype calling - Feb 26, 2018

Goals: 
1. Take sam files and extract mapping stats and read depth coverage
2. Convert sam to binary (bam) format, and use samtools/bcftools for SNP detection
3. Learn Variant Call Format (vcf) for storing SNP genotypes
4. Begin to evaluate filtering strategies for determining high quality SNPs for downsterm analyses

First: call SNPs using the program samtools (manual can be found [here](http://www.htslib.org/doc/samtools.html)

Samtools: a powerful tool for manipulating sam files (and their binary equivalent, bam), and for piling up those reads across individuals to call SNPs and genotypes

For individual sam files from RNASeq mapping- Use samtools to: 
1. convert from sam >> bam
2. check mapping stats
3. Fix reads that are no longer paired
4. Remove duplicates
5. Index for computational efficiency
6. Use bcftools to call SNPS and genotypes

There is information in our alignments that will be important for analyzing the SNP data downstream: 
- Position: where is the SNP located within a contig or chromosome of the reference assembly? 
- Alleles: what are the alleles present at a given SNP? Are there only 2, or are there more? Are the single-nucleotide differences? Which SNPs are sequencing errors and which are real. 
- Depth: How many reads cover a given SNP? How many reads were observed for each allele? 
- Genotype QUality (GQ): How confident are we that we're calling the correct genotype (ex, AA, AT, or TT)? How can we tell a heterozygote from sequencing error? 
navigate to server mydata/sam
open sam folder

Variant Call Format (VCF) for SNP data
- Used as the common standard to store polymorphic sites only in a new file
- [Here](https://github.com/samtools/hts-specs/blob/master/VCFv4.3.pdf) is the link to the description of what each field in a vcf file means
- First step for learning how to work with vcf files is to use the program VCFTools to parse data files into just those samples and sites of interest, and calculate diversity stats on these
- [Here](https://vcftools.github.io/man_latest.html) is the manual page for VCFTools
- We will be using VCFTools to examine the effects of different filtering strategies on the number of SNPs that we get and their quality. 

```
in bash: file saved as samtobam.sh in mydata/sam
- make bash file executable using chmod o+x
- go into screen - used to create a connection between terminal and computer, so that you can disconnect from terminal and come back later, or run a script in the background
- run bash script using bash samtobam.sh
- use ctrl + a + d command to detach from script
```
* Step 1: to see if VCFtools likes our file format, and get some basic info on the number of SNPs and samples
```
 vcftools --vcf WA_PP1_M1.vcf
```
Step 2: Try filtering out positions that are likely to be errors in the sequencing or genotyping process - try to identify how many SNPs would pass each filter without changing the data file at all, and then decide what combination of filters we want to implement. 
* Keep sites with only 2 alleles (biallelic, not multi-allelic SNPs) because when looking at within species diversity, very rare to have mutations occur at the same position

```
vcftools --vcf WA_PP1_M1.vcf --min-alleles 2 --max-alleles 2

Parameters as interpreted:
        --vcf WA_PP1_M1.vcf
        --max-alleles 2
        --min-alleles 2

After filtering, kept 1 out of 1 Individuals
After filtering, kept 225131 out of a possible 226131 Sites
```
* Genotype quality (GQ): treat genotypes below a certain (phred-scaled) likelihood threshold as "missing". Even wen the average read depth is low, the GQ scores provide useful information on the multinomial likelihood that the called genotype is correct. These are expressed in phred-scaled quantitied (GQ = -10 x log10(Probability of G)). Start with GQ threshold of 10 (90% confidence)

```
vcftools --vcf WA_PP1_M1.vcf --minGQ 10

Parameters as interpreted:
        --vcf WA_PP1_M1.vcf
        --minGQ 10

After filtering, kept 1 out of 1 Individuals
After filtering, kept 226131 out of a possible 226131 Sites
```
* Missing data across individuals: get rid of sites where fewer than 50% of our samples have data
 + Rationale: Missing data is a problem for any analysis,a nd population genetic statistics can behave oddly (ie, become biased) when a lot of individuals are msising data for a given SNP. 
 ```
 vcftools --vcf WA_PP1_M1.vcf --max-missing 0.5
```
* Minor allele count (MAC): gets rid of very rare SNPS, based on a user-defined threshold
  + Rationale: sequencing errors are relatively common, but they tend to happen randomly and affect only 1 read at a time. Therefore, if we have a SNP that is only seen very rareely, it might be a sequencing error, and should be discarded. For us, the most liberal filters would be having a minimum of 2 copies of the minor allele present out of the total 2N copies. 
```
vcftools --vcf WA_PP1_M1.vcf --mac 2
```
* Combining filters: 
```
vcftools --vcf WA_PP1_M1.vcf --min-alleles 2 --max-alleles 2 --minGQ 10 --max-missing 0.5 --mac 2 
```
**Evaluating the influence of genotype uncertainty on inferences of genetic diversity**
- VCFtools can also provide output in the form of many useful summary stats on a vcf file. We'd like to know how sensitive our inferences of diversity are to the range of GQ values present among SNP sites, which directly relates to how much confidence we have that a genotype has been called correctly. 
- We can write a bash script with a for-loop to run vcftools iteratively for each of a range of GQ values. UNIX will take a continuous range of avariable and process it in a loop using curly brackets {10..25} or for a fixed set of values 10 15 20 25
- Look at the influence of GQ filtering on the homozyogisity inferred for each individual (F). VCFtools lets us quickly calculate this using the -het option. Put it into a loop and let our bash script do the work for us. 

**Reads2Snps**
- Alternative SNP caller to samtools
- Applied following criteria: 
  + Minimum depth to call a genotype = 5 reads
  + Minium posterior probability = 0.9
- SNPs that didn't meet the criteria were flagged as unres (unresolved) and set to missing data in the vcf file
- Loci that showed evidence of paralogy flagged as para

------
<div id='id-section11'/>
### Page 11: Population Genomics 2: Diversity and Site Frequency Spectrum (SFS) - February 28, 2018
Goals: 
* Learn to calculate SNP allele frequencies and diversity metrics with vcftools
* Learn to subset the vcf file for interesting individuals (or for specific chromosomes or SNP loci)
* Calculate nucleotide diversity (pi) for each contig

Getting summary stats for downstream analysis and plotting in R
- We now have a filtered SNP dataset that has high-quality sites in it, so time to look at some different measures of genetic diversity in our sample. 
- The diverisy of a population primarily reflects its **effective population size (Ne)**
  + Ne is shaped by many different aspects of species life history and ecology, including sex ratio, generation time, mating system, offspring number, etc
  +  Also by the populations history, including bottlenetcks and population growth
- Therefore, looking at the diversity within populations, and comparing to other populations or species, is an important step towards understanding how ecology shapes genomes
- There are many different ways to loo at the diversity within populations using SNPs: 
  + Nucleotide diversity (pi): the average number of pairwise differences between all individuals in the population. Equivalent to the expected heterozygosity (2pq) for a bi-allelic locus. 
  + Allele frequencies (P and q): the frequency of a given SNP. Usually defined in terms of the major and minor alleles at each SNP locus. 
  + Site Frequency Spectrum (SFS): aka allele frequency spectrum. The histogram of allele frequencies across all SNP loci. Tells how many loci are rare (frequency in population 0-.1) and common (.4-.5). The shape of the SFS has an incredible amount of information about the populations demographic history (ie Ne, size changes) and also selection (purifying and positive selection)
  
** Diversity metrics based on subsetting your VCF files**
- Often useful to subset the total SNP dataset to analyze diversity in different groups
- To do this, create a separate text file containing a subset of the samples so vcftools can be told how to split things up. 
Example: compare the SNP frequencies for all loci between our 3 populations. 
1. Create text files containing the individual ID's for each population separately. Path to the metadata: 
```
/data/project_data/beetles.metadata/cols_data.txt
```
2. Get just the IT, NC, and WA sample IDs, and remove all but the first column of data
```
grep "IT" /data/project_data/beetles/metadata/cols_data.txt | cut -f 1 > IT.inds
```
3. Now that individuals are separated out by population, we can call vcftools to calculate alleles frequencies separately for each population (3 separate calls to vcftools)
```
vcftools --vcf filename.vcf --freq 2 --keep IT.inds --out IT
```
4. Move results files to laptops using WinSCP
5. Make SFS for the three populations using R
```
hist()
```

**Estimating Onthofagus nucleotide diversity**
- There are associations between species life history traits and nucleotide diversity
- pi is proportional to Ne, through the following equation, where 'u' is the per-site mutation rate:
```
  -> pi = 4 Ne u
```  
- We can use vcftools to calcualte pi for each site, and then aggregate the data per contig in R to get mean values. 
- Because pi includes both polymorphic and monomorphic sites in the genome, we'll need to use a filtering strategy on our vcf file that includes the non-variable sites

- open samtobam.sh
- comment out all lines run in previous session
```
#samtools depth WA_PP1_M1_bwamem.bam

# fix mate pairs between PE reads that have been orphaned
#samtools fixmate WA_PP1_M1_bwamem.bam WA_PP1_F1.fixmate.bam

# sort the alignments
#samtools sort -@ 4 WA_PP1_F1.fixmate.bam WA_PP1_F1.fixmate.sorted.bam

# remove PCR duplicates
# samtools rmdup WA_PP1_F1.fixmate.sorted.bam WA_PP1_F1.fixmate.sorted.rmdup.bam

# index our final alignment for fast processing
#samtools index WA_PP1_F1.fixmate.sorted.bam

# pause here: look at the results before proceeding to SNP calling

# here, we're calling SNPs in one individual using the samtools bcftools caller
bcftools mpileup \
  -Ou -f /data/project_data/beetles/reference/OTAU.fna \
  ~/mydata/sam/WA_PP1_F1.fixmate.sorted.bam \
  --threads 4 --skip-indel --annotate AD,DP | \
  bcftools call -Ov -mv --format-fields GQ,GP >WA_PP1_M1.vcf
"samtobam.sh" 34L, 1098C 
```
- error: WA_PP1_M1 accidentally renamed to WA_PP1_F1 partway thru script in previous session - fixed here by making bcftools output (when calling SNPs) with correct name
- | pipeline command in bash that puts output from previous command as input into next
- / allows splitting of command between multiple lines

- VCF = variant calling format
  - columns associated with the number of samples in the data
  - rows associated with SNPs present

- analyzing .vcf file
 - head to check top
 - wc to check length
 - use tail to look at opposite end of file
 - then open .vcf file using vim filename
 - jump to a given line using :number
 - and set no wrap using : set nowrap
 
 PART 2: 
 - navigate to /data/project_data/beetles/snps
 - these files contain data for all individuals
 - minDP (minimum depth to call SNPs) = 5
 - minGP (minimum genotype probability) = 0.9
 
 - make new folder called myresults in home directory, to receive output of analysis from shared directory
 - vcftools: --command format
 
----------
<div id='id-section12'/>
### Page 12: Admixture & Population Structure - March 5, 2018

Goals: 
- Look for evidence of genetic population structure using ADMIXTURE analysis
- Compare ADMIXTURE results to multivariate ordination techniques such as PCA to detect population structure. 

We'll take 2 different approaches to test if there is any population structure present in our sample. Both analyses are naive with regard to the actual sampling locality of individuals, so they provide a relatively unbiased way of determining if there are actually >1 genetically distinct groups represented in the data. 
  1. The maximum likelihood ADMIXTURE program to cluster genotypes into K groups, in which we'll vary K from 1-10
  2. PCA and related analyses on the SNPs to see if they group by population. 

**ADMIXTURE analysis**
- A common way to estimate population structure or test for mixed ancestry is to use genotypic clustering algorithms
- These include the familiar program STRUCTURe, as well as many others. All include the common feature of using multi-locus genetic data to estimate: 
  1. The number of clusters present
  2. Each individuals proprtion of genetic ancestry in these clusters

- With large population genomic datasets, STRUCTURE would take a prohibitively long time to run, so analyzing thousands of SNPS requires computationally efficient approaches to the clustering problem. A good option is the maximum likelihood program ADMIXTURE by the NOvembre lab. [Here](https://www.genetics.ucla.edu/software/admixture/) is the source page for ADMIXTURE information, and [here](https://www.genetics.ucla.edu/software/admixture/admixture-manual.pdf) is the ADMIXTURE manual. 
- ADMIXTURE introduces a number of user-defined groups or clusters (K) and uses maximum likelihood to estimate allele frequencies in each cluster, and assign each individual ancestry (Q) to one or more of these clusters. 

To run ADMIXTURE, we need to provide an input file and the requested level of K to investigate. 
1. Convert vcf >.geno format using PGDSpider, a conversion tool. Requires 4 files: 
  + The input data file in vcf format (thinned to remove closely linked sites in LD)
  + A text file with sample IDs and the population designations
  + A settings file (.spid) that tells PGDSpider how to process the data
  + A bash script that runs the program with all of the above settings specified. 
*Files can be accssed here: 
```
/data/project_data/beetles/metadata/beetles.pop
/data/project_data/scripts/beetles.spid
/data/project_data/scripts/vcf2geno.sh
```

In server: access data/project_data/beetles/snps
Use vcftools to analyze vcf files produced by reads2snps
```
vcftools --vcf OTAU_2018_reads2snps_DP10GP95.vcf --min-alleles 2 --max-alleles 2 --maf 0.01 --max-missing 0.8 --het --out ~/myresults/reads2snpsmiss0.8
```
Use R in server to read in data, and calculate summary and df of F
- F represents the inbreeding coefficent (heterozygosity in individual relative to subpopulation). Low inbreeding coefficient = excess of heterozygotes, high inbreeding coefficent = excess of homozygotes
- Actual value depends on biology and life histroy strategies of organisms

```
>R
> df <- read.table("reads2snpsmiss0.8.het", header=T)
> head(df)
        INDV O.HOM.  E.HOM. N_SITES        F
1 IT_AD4_F1_  98985 98336.4  127352  0.02235
2 IT_AD4_F2_  88580 90908.7  117665 -0.08704
3 IT_AD4_F3_  99355 99213.6  128482  0.00483
4 IT_AD4_M1_  92147 90344.0  116911  0.06787
5 IT_AD4_M2_  93096 97362.4  125674 -0.15070
6 IT_AD4_M3_ 100851 99243.7  128559  0.05483
```
```
> summary(df$F)
    Min.  1st Qu.   Median     Mean  3rd Qu.     Max.
-0.15070  0.02253  0.06285  0.05485  0.09509  0.16704
```

```
> sd(df$F)
[1] 0.06320409
```
Now run analysis using chosen model and parameters (reads2snps, max missing = 0.8)

```
vcftools --vcf OTAU_2018_reads2snps_DP10GP95.vcf --min-alleles 2 --max-alleles 2 --maf 0.01 --max-missing 0.8 --recode --out ~/myresults/OTAU_2018_reads2snps_DP10GP95_biallelic_MAF01_Miss0.8.vcf
```

Now pull out from vcf file individual populations
```
grep "IT" /data/project_data/beetles/metadata/cols_data.txt | cut -f 1 > IT.inds
```
And now 
```
vcftools --vcf OTAU_2018_reads2snps_DP10GP95_biallelic_MAF01_Miss0.8.vcf.recode.vcf --keep IT.inds --freq2 --out IT

Parameters as interpreted:
        --vcf OTAU_2018_reads2snps_DP10GP95_biallelic_MAF01_Miss0.8.vcf.recode.vcf
        --keep IT.inds
        --freq2
        --out IT

Keeping individuals in 'keep' list
After filtering, kept 24 out of 72 Individuals
Outputting Frequency Statistics...
After filtering, kept 8730 out of a possible 8730 Sites
Run Time = 0.00 seconds

```

------
<div id='id-section13'/>
### Page 13: Admixture & Population Structure Continued - March 7, 2018

**Thinning your vcf file for ADMIXTURE**
- ADMIXTURE assumes SNPs are unlinked, so we first need to thin the data for closely adjacent sites
- Use the vcftools flag --thin 1000 to thin sites to 1 SNP per kb
- You should have a file called out.recode.vcf.geno in your directory
- From within your home directory ~/myscripts/, open vim to create a bash script for running ADMIXTURE at each level of K from 1 to 10. 
- The cross-validation procedure in ADMIXTURE breaks the samples into n equally sized chunks. It then masks each chunk in turn, trains the model to estimate the allele frequencies and the ancestry assignments on the unmasked data, and then attempts to predict the genotype values for the masked individuals
- If the model is good, and there is true structure in the data, then a supported value of K will show low CV error. BUT, this does not mean there is only 1 true K value!!

** Notes on implemenation**
- First, repeat steps from session 12 for NC & WA populations on server
- Next, load data into new R script (March7_PopGen.R) and calculate SFS for each population

next: 
- thin vcf file, convert vcf to .geno, write bash script and run admixture
```
vcftools --vcf OTAU_2018_reads2snps_DP10GP95_biallelic_MAF01_Miss0.8.vcf.recode.vcf --thin 1000 --recode
```
- file is different than what other people have, so go back to last session and overwrite .vcf.recode.vcf file

- then move three PGD spider files (beetle.pop, beetle.spid, and bash script) to ~/myscripts

- edit beetle.pop and beetle.spid using vim script name
- try to run script in bash, not working, so: 
- access copy of .geno file  in /data/project_data/beetles/snps, move to /myresults

- make new bash script (ADMIX.sh) to run for loop with different values of K for admixture analysis

------
<div id='id-section14'/>
### Page 14: HW-1 R Script

2 RScripts used: One to analyze differential gene expression between populations as a whole (HW1_wholePop), and one to analyze differential expression between each gender of each population separately (HW1_sexPop). 

------
<div id='id-section15'/>
### Page 15: Population Genomics 4: Admixture & Population Structure Continued - March 19, 2018
Goals (finishing up from Day 4): 
- Plot our ancestry assignments from ADMIXTURE analysis for different levels of K
- Compare ADMIXTURE results to multivariate ordination techniques such as PCA to detect population structure

In UNIX: 
1. Convert vcf to .geno format in preparation for ADMIXTURE
  + edit PGDSpider settings (.spid) files
  + edit bash script (vcf2geno.sh)
  + run converstion vcf -> geno
2. Run ADMIXTURE
  + Write ADMIX.sh bash script to loop the ADMIXTURE analysis from K=1 to K=10
  + look at cv values to find evidence for K
  + download .Q files to local desktop for R-processing
  + download metadata "cols_data.txt" to local desktop 
  + download thinned vcf file to local desktop for R-processing

** Notes on implementation**
1. Move beetle.spid file & beetle.pop file over to ~/myscripts
2. Check beetle.spid file & vcf2geno.sh file in bash 
```
vim beetle.spid
vim vcf2geno.sh
```
run vcf2geno.sh using bash

```
vim ADMIX.sh

for K in {1..10}
do
admixture -j4 --cv=10 ~/myresults/out.recode.vcf.geno  $K | tee  log${K}.out
done
grep "CV" log*out >chooseK.txt
```
```
bash vcf2geno.sh
```

output file is out.recode.vcf.geno, composed of matrix of 4 numbers (REf= A, Alt = T)
- 0 = AA
- 1 = AT
- 2 = TT
- 9 = ""

Next: check ADMIX.sh file using vim
- functions as a for loop, running through 1-10 K values
- using 10 groups for cross validation statistic
- takes out.recode.vcf.geno file, for each value of K
- searches for CV values and outputs it as .txt file

run ADMIX.sh file using bash
output is CV errors for each K value
```
[msouthga@pbio381 myresults]$ cat chooseK.txt
log10.out:CV error (K=10): 0.74078
log1.out:CV error (K=1): 0.44089
log2.out:CV error (K=2): 0.43635
log3.out:CV error (K=3): 0.44473
log4.out:CV error (K=4): 0.47987
log5.out:CV error (K=5): 0.51791
log6.out:CV error (K=6): 0.55534
log7.out:CV error (K=7): 0.58280
log8.out:CV error (K=8): 0.62599
log9.out:CV error (K=9): 0.66978
```
Choose K values 2-4

.P = allele fragements
.Q = ancestry coefficeints
.log = logged output

NEXT: move .Q files to home computer, start new R script (PopStructureOTAU_DP10GP95.R)

** In R on Local Machine**
1. Plot ADMIXTURE Q-values
  + use 'pophelper'package to plot ancestry coefficients by population
  + compare ancestry values across levels of K
2. Compare ADMIXTURE results to PCA results
  + Use 'vcfR' to read in vcf file to R
  + use 'adegenet' to compute PCA on SNP data
  + How many genetic groups does PCA suggest? Does it agree with ADMIXTURE? 

------
<div id='id-section16'/>
### Page 16:Testing Selection Using Fst outlier analysis
- Using the program Bayescan to test for SNPS with very high Fst, indicative of the action of diversifying selection and local adaptation
- Estimates two key parameters: 
- Most important parameter is -pr_odds: sets the prior probability on the odds that a given SNP is neutral vs. under selection

- For class analysis: take 10 threads values

------
<div id='id-section17'/>
### Page 17: Population Genomics 7 - Analyzing Fst Outlier (Bayescan) Results - March 26, 2018

Goals
- Discuss mcmc approach used by Bayescan to estimate model parameters
- Assess convergence of our mcmc runs in Bayescan and the effects of different thinnning intervals
- Compare the sensitivity of outlier loci from Bayescan runs assuming different prior odds of selection
- Annotate our results to determine the functional role of outliers

Plotting and Interpreting Results
- Bayescan and other evolutionary genetic models have a tough job - trying to estimate complex models with many parameters
- Often, we don't know what the distribution should look like for these set of parameters, so we approximate the likelihood by sampling the space most consistent with the data. Thats where mcmc comes in - it's a search strategy to for finding regions of high likelihood, and uses this to compute your posterior parameters 

Issues to be aware of when using mcmc programs: 
1. **Burn-in**: The time before a stable sampling distribution of high likelihood values is reached. The initial start of the chain can bias your estimates if you include these values. People generally discard the first several thousand iterations as "burn-in"
2. **Thinning**: Saving iterations that are too close together in the chain generates autocorrelation in your estimates, which can bias your parameters
3. **Convergence**: Is there a trend in the likelihood or parameter values across the length of the chain? Or has it reached a stable convergence of sampling around likely values? 

**Most importantly - do not trust the output of ANY program. You have to look at your data and results to ensure things are OK.**

Hands-on analysis
Here's what's in these files:

- AccRte.txt: This contains information on the acceptance rate for updates during the mcmc chain.
- fst.txt: This a row for each SNP site, and contains the probability that it's under selection, the log10
- Posterior Odds that the locus is selected vs. neutral, the value of alpha, the corresponding qvalue indicating the probability of a false discovery, and lastly, the Fst for that locus
- .sel: This contains the output from the mcmc chain; can be useful for plotting and ensuring your run converged (look at logL)
- Verif.txt: This is essentially a log file that contains information on your run and how the commands were interpreted by Bayescan.

**The most important files are the "_fst.txt" with your selection results, and the ".sel" with the info on how the likelihood varied across your run.**

Analysis
1. Download the "_fst.txt" and the ".sel" files to local machine for plotting in R
2. Download the SNP ID information (CHROM and POS) so that you can reassociate Go to ~/myresults and analyze .vcf file
```
ll *vcf

vcftools --vcf OTAU_2018_reads2snps_DP10GP95_biallelic_MAF01_Miss0.8.vcf.recode.vcf --kept-sites

```
output is out.kept.sites, download to home machine

3. Start new R script (~/EcologicalGenomics/PopGenomics/BayescanResults/BayescanAnalysis.R)


------
<div id='id-section18'/>
### Page 18: Population Genomics 7 Continued

- load in other finished .sel and .fst.txt files from server
- working in BayescanAnalysis.R

- write list of chromosomes out as .txt file (bayescan_pr10_candsnps.txt) and save in ~/myresults on server
```
uniq bayescan_pr10_candsnps.txt > bayescan_pr10_candsnps_unique.txt

 wc bayescan_pr10_candsnps.txt
  76   76 1140 bayescan_pr10_candsnps.txt

 wc bayescan_pr10_candsnps_unique.txt
 46  46 690 bayescan_pr10_candsnps_unique.txt

grep -f ~/myresults/bayescan_pr10_candsnps.unique.txt OTAU.Models.gff3 | grep "gene" >~/myresults/Bayescan_outlier_genes.txt
```


------
<div id='id-section19'/>
### Page 19: Work on dem history proj

IN server: access myresults 

--thin filtered vcf file to 1 SNP per 1000 bp--
```
vcftools --vcf OTAU_2018_reads2snps_DP10GP95_biallelic_MAF01_Miss0.8.vcf.recode.vcf --thin 1000 --recode
```
--output: out.log & out.recode.vcf

# now subset by each population
```
vcftools --vcf out.recode.vcf --keep IT.inds --recode --out ITdem
```
```
vcftools --vcf out.recode.vcf --keep NC.inds --recode --out NCde
```
```
vcftools --vcf out.recode.vcf --keep WA.inds --recode --out WAdem
```
output is (popname)dem.recode.vcf

- made new project folder in /data/project_data/beetles/share/DemHist

- transfer subsetted vcf files to shared DemHist folder 
cp *dem *vcf /data/project_data/beetles/share/DemHist

- make list file for IT called listfileIT by copying over info from IT.inds filell

- run pl script to convert vcf file to dadi input (data dictionary)


Next: write purl script to convert data dictionary to a frequency spectra (fs)

------
<div id='id-section20'/>
### Page 20: 2018-04-18 - Work on Demographic History of Beetles Project

- start from data dictionary (ITdem.recode.vcf.data) and try to convert it to fs using fs_from_data_IT.py -> works but unable to plot using

-use fs.S() to return the number of segregating sites, maximized at projections = 27 (27 individuals???)
(9486 max number of segregating sites)

- 

------
<div id='id-section21'/>
### Page 21: April 23, 2018 - Continuing work on DemHist project

Goal: run dadi model for IT population

dadi on server: /data/popgen/dadi_2018

* created fs file for IT called IT.fs using fs.to_file function in fs_from_data_IT.py file

* remove spaces at end of IT listfile and re-run all current analyses
* stil returns sample size as 27, probably because we specified the projection at 27??? 

* set the points to [50,60,70]

RUNNING FIRST MODEL - Standard neutral model with IT, returned maximum log composite likelihood of 2465.12

```
./convert_vcf_to_dadi_input.pl

```
- convert other vcf populations files to dadi input
```
perl convert_vcf_to_dadi_input.pl<filename><listfilename>
```


------
<div id='id-section22'/>
### Page 22: WOrk on DemHist project continued


Move the .vcf files generated to shared folder, delete old ones, renamed to be consistent with what we called them in the scripts (ITdem.recode.vcf)
- Also deleted all .fs files and .data files to re-run analyses

Re-run conversion for all three populations using convert_vcf_to_dadi_input.pl, output is *popname*dem.recode.vcf.data

- outputted new txt file with loci names containing columns one and 2 - called bayescan_pr10_candsnps2.txt

```
vcftools --vcf OTAU_2018_reads2snps_DP10GP95_biallelic_MAF01_Miss0.8.vcf.recode.vcf --exclude-positions bayescan_pr10_candsnps2.txt --recode --out OTAU_2018_reads2snps_pr10exclude.vcf

```

Then re-run fs_from_data.py scripts for each population

And re-run

FIrst: resubset unthinned vcf files by populations
```
vcftools --vcf OTAU_2018_reads2snps_pr10exclude.vcf --keep IT.inds --recode --out ITdem2

vcftools --vcf OTAU_2018_reads2snps_DP10GP95_biallelic_MAF01_Miss0.8.vcf.recode.PR_10.vcf --keep NC.inds --recode --out NCdem2

vcftools --vcf OTAU_2018_reads2snps_DP10GP95_biallelic_MAF01_Miss0.8.vcf.recode.PR_10.vcf --keep WA.inds --recode --out WAdem2
```
------
<div id='id-section23'/>
### Page 23:
------
<div id='id-section24'/>
### Page 24:
------
<div id='id-section25'/>
### Page 25:
------
<div id='id-section26'/>
### Page 26:
------
<div id='id-section27'/>
### Page 27:
------
<div id='id-section28'/>
### Page 28:
------
<div id='id-section29'/>
### Page 29:
------
<div id='id-section30'/>
### Page 30:
------
<div id='id-section31'/>
### Page 31:
------
<div id='id-section32'/>
### Page 32:
------
<div id='id-section33'/>
### Page 33:
------
<div id='id-section34'/>
### Page 34:
------
<div id='id-section35'/>
### Page 35:
------
<div id='id-section36'/>
### Page 36:
------
<div id='id-section37'/>
### Page 37:
------
<div id='id-section38'/>
### Page 38:
------
<div id='id-section39'/>
### Page 39:
------
<div id='id-section40'/>
### Page 40:
------
<div id='id-section41'/>
### Page 41:
------
<div id='id-section42'/>
### Page 42:
------
<div id='id-section43'/>
### Page 43:
------
<div id='id-section44'/>
### Page 44:
------
<div id='id-section45'/>
### Page 45:
------
<div id='id-section46'/>
### Page 46:
------
<div id='id-section47'/>
### Page 47:
------
<div id='id-section48'/>
### Page 48:
------
<div id='id-section49'/>
### Page 49:
------
<div id='id-section50'/>
### Page 50:
------
<div id='id-section51'/>
### Page 51:
------
<div id='id-section52'/>
### Page 52:
------
<div id='id-section53'/>
### Page 53:
------
<div id='id-section54'/>
### Page 54:
------
<div id='id-section55'/>
### Page 55:
------
<div id='id-section56'/>
### Page 56:
------
<div id='id-section57'/>
### Page 57:
------
<div id='id-section58'/>
### Page 58:
------
<div id='id-section59'/>
### Page 59:
------
<div id='id-section60'/>
### Page 60:

------
